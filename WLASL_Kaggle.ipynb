{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stuartisteefanos17/Sign-Language-Deep-Learning-Repo/blob/main/WLASL_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs_cZ3ZU0aFJ"
      },
      "source": [
        "# Link data and extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipOr_jgMz2en"
      },
      "source": [
        "Load Kaggle data to Colab:\n",
        "- https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/\n",
        "- https://www.kaggle.com/general/74235\n",
        "\n",
        "Dataset link:\n",
        "- https://www.kaggle.com/datasets/risangbaskoro/wlasl-processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx1b_9mKuCAK"
      },
      "outputs": [],
      "source": [
        "pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "height": 38
        },
        "id": "9DDaHPyUvs72",
        "outputId": "720e039a-2bf3-4bae-ca98-5b58b8b4193f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a8891d7-d747-470b-bd32-5040128078ec\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a8891d7-d747-470b-bd32-5040128078ec\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "iWBH1GiVgLV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "dxN9DbR4gQz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "UNfGyA83gVEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "40m1AANXga5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "XV8b1Aqtge8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "INHNyqi8gj2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvQ4kpyExxH7"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stLRJzB8xY_Y"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbRZMTMhx7nV"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAB77uhcyBQ7"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcbF95wHyw5F"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download risangbaskoro/wlasl-processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPllSco8y8mv"
      },
      "outputs": [],
      "source": [
        "! unzip wlasl-processed.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoyFEq50ijA"
      },
      "source": [
        "# Playing around with the new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzLZ4u1r0k74"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z5Jh29cYY8K"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade youtube-dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJGjVJARaVRD"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl9RnAe7aZp4"
      },
      "outputs": [],
      "source": [
        "# cd /content/drive/MyDrive/SI Project - Code and Datasets/WLASL (World Level American Sign Language) Video/start_kit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM4GtFsNavZf"
      },
      "outputs": [],
      "source": [
        "# !python \"/content/drive/MyDrive/SI Project - Code and Datasets/WLASL (World Level American Sign Language) Video/start_kit/video_downloader.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJCrfBA0bpV8"
      },
      "outputs": [],
      "source": [
        "# !python \"/content/drive/MyDrive/SI Project - Code and Datasets/WLASL (World Level American Sign Language) Video/start_kit/preprocess.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-QDb8yi2sX3"
      },
      "outputs": [],
      "source": [
        "main_path = '/content/WLASL_v0.3.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHSnA6B_0lp2"
      },
      "outputs": [],
      "source": [
        "# load the dataset into a pandas dataframe (this will help us to easy manipulate the data)\n",
        "wlas_df = pd.read_json('/content/WLASL_v0.3.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o97CeEq1GPV"
      },
      "outputs": [],
      "source": [
        "wlas_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdE5Xb5LrUJe"
      },
      "outputs": [],
      "source": [
        "# for i in range(2000):\n",
        "#   print(len(wlas_df[\"instances\"][i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc3m1xns1Z8S"
      },
      "outputs": [],
      "source": [
        "def get_videos_ids(json_list):\n",
        "    \"\"\"\n",
        "    FIXED!\n",
        "    function to check if the video id is available in the dataset\n",
        "    and return the viedos ids of the current instance\n",
        "    \n",
        "    input: instance json list\n",
        "    output: list of videos_ids\n",
        "    \n",
        "    \"\"\"\n",
        "    with open(main_path, 'r') as data_file:\n",
        "      json_data = data_file.read()\n",
        "\n",
        "    instance_json = json.loads(json_data)\n",
        "    videos_list = []    \n",
        "    \n",
        "    for word in instance_json:\n",
        "      for vid in word['instances']:\n",
        "        video_id = vid['video_id']\n",
        "        if os.path.exists(f'/content/videos/{video_id}.mp4'):\n",
        "            videos_list.append(video_id)\n",
        "    return videos_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F1Tz_Wo1Lr2"
      },
      "outputs": [],
      "source": [
        "videos_list = get_videos_ids(main_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8PXNT7F1tpD"
      },
      "outputs": [],
      "source": [
        "def get_json_features(json_list):\n",
        "    \"\"\"\n",
        "    DO NOT USE!\n",
        "    function to check if the video id is available in the dataset\n",
        "    and return the viedos ids and url or any other featrue of the current instance\n",
        "    \n",
        "    input: instance json list\n",
        "    output: list of videos_ids\n",
        "    \n",
        "    \"\"\"\n",
        "    with open(main_path, 'r') as data_file:\n",
        "      json_data = data_file.read()\n",
        "    \n",
        "    instance_json = json.loads(json_data)\n",
        "\n",
        "    videos_ids = []\n",
        "    videos_urls = []\n",
        "\n",
        "    for word in instance_json:\n",
        "      for vid in word['instances']:\n",
        "        video_id = vid['video_id']\n",
        "        video_url = vid['url']\n",
        "        if os.path.exists(f'/content/videos/{video_id}.mp4'):\n",
        "            videos_ids.append(video_id)\n",
        "            videos_urls.append(video_url)\n",
        "    return videos_ids, videos_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU8E4TA74HTg"
      },
      "outputs": [],
      "source": [
        "videos_ids, videos_urls = get_json_features(main_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiaOmPIV4zpG"
      },
      "outputs": [],
      "source": [
        "wlas_df['videos_ids'] = wlas_df['instances'].apply(get_videos_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuDhHLxA48aF"
      },
      "outputs": [],
      "source": [
        "wlas_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZoQrt96H8h"
      },
      "source": [
        "## Features dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOV1lnSI6Las"
      },
      "outputs": [],
      "source": [
        "features_df = pd.DataFrame(columns=['gloss', 'video_id', 'url'])\n",
        "for row in wlas_df.iterrows():\n",
        "#     print(row[1][1])\n",
        "    ids, urls = get_json_features(row[1][1])\n",
        "    word = [row[1][0]] * len(ids)\n",
        "    df = pd.DataFrame(list(zip(word, ids, urls)), columns = features_df.columns)\n",
        "    features_df = features_df.append(df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHyNtr_f8Ceq"
      },
      "outputs": [],
      "source": [
        "features_df.index.name = 'index'\n",
        "features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxWIsip88Te5"
      },
      "source": [
        "# Training code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easydict"
      ],
      "metadata": {
        "id": "QtRwGBlKNeyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config"
      ],
      "metadata": {
        "id": "tmvInABGTJNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehJeqBhu8VqX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import easydict\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as IT\n",
        "from torchvision.transforms import Compose\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from sys import argv\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "#import videotransforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from config import Config\n",
        "from pytorch_i3d import InceptionI3d\n",
        "from easydict import EasyDict\n",
        "\n",
        "# from datasets.nslt_dataset import NSLT as Dataset\n",
        "from nslt_dataset import NSLT as Dataset\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "parser.parse_known_args()[0]\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_known_args()[0]\n",
        "args = easydict.EasyDict({\n",
        "    \"batch_size\": 100,\n",
        "    \"train_steps\": 1000\n",
        "})\n",
        "#args = parser.parse_args(argv[1:])\n",
        "\n",
        "parser.add_argument('-mode', type=str, help='rgb or flow')\n",
        "parser.add_argument('-save_model', type=str)\n",
        "parser.add_argument('-root', type=str)\n",
        "parser.add_argument('--num_class', type=int)\n",
        "\n",
        "#args = parser.parse_args()\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_cn85T08kj-"
      },
      "outputs": [],
      "source": [
        "def run(configs,\n",
        "        mode='rgb',\n",
        "        root='/ssd/Charades_v1_rgb',\n",
        "        train_split='charades/charades.json',\n",
        "        save_model='',\n",
        "        weights=None):\n",
        "    print(configs)\n",
        "\n",
        "    # setup dataset\n",
        "    train_transforms = transforms.Compose([videotransforms.RandomCrop(224),\n",
        "                                           videotransforms.RandomHorizontalFlip(), ])\n",
        "    test_transforms = transforms.Compose([videotransforms.CenterCrop(224)])\n",
        "\n",
        "    dataset = Dataset(train_split, 'train', root, mode, train_transforms)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=configs.batch_size, shuffle=True, num_workers=0,\n",
        "                                             pin_memory=True)\n",
        "\n",
        "    val_dataset = Dataset(train_split, 'test', root, mode, test_transforms)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=configs.batch_size, shuffle=True, num_workers=2,\n",
        "                                                 pin_memory=False)\n",
        "\n",
        "    dataloaders = {'train': dataloader, 'test': val_dataloader}\n",
        "    datasets = {'train': dataset, 'test': val_dataset}\n",
        "\n",
        "    # setup the model\n",
        "    if mode == 'flow':\n",
        "        i3d = InceptionI3d(400, in_channels=2)\n",
        "        i3d.load_state_dict(torch.load('weights/flow_imagenet.pt'))\n",
        "    else:\n",
        "        i3d = InceptionI3d(400, in_channels=3)\n",
        "        i3d.load_state_dict(torch.load('weights/rgb_imagenet.pt'))\n",
        "\n",
        "    num_classes = dataset.num_classes\n",
        "    i3d.replace_logits(num_classes)\n",
        "\n",
        "    if weights:\n",
        "        print('loading weights {}'.format(weights))\n",
        "        i3d.load_state_dict(torch.load(weights))\n",
        "\n",
        "    i3d.cuda()\n",
        "    i3d = nn.DataParallel(i3d)\n",
        "\n",
        "    lr = configs.init_lr\n",
        "    weight_decay = configs.adam_weight_decay\n",
        "    optimizer = optim.Adam(i3d.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    num_steps_per_update = configs.update_per_step  # accum gradient\n",
        "    steps = 0\n",
        "    epoch = 0\n",
        "\n",
        "    best_val_score = 0\n",
        "    # train it\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3)\n",
        "    while steps < configs.max_steps and epoch < 400:  # for epoch in range(num_epochs):\n",
        "        print('Step {}/{}'.format(steps, configs.max_steps))\n",
        "        print('-' * 10)\n",
        "\n",
        "        epoch += 1\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            collected_vids = []\n",
        "\n",
        "            if phase == 'train':\n",
        "                i3d.train(True)\n",
        "            else:\n",
        "                i3d.train(False)  # Set model to evaluate mode\n",
        "\n",
        "            tot_loss = 0.0\n",
        "            tot_loc_loss = 0.0\n",
        "            tot_cls_loss = 0.0\n",
        "            num_iter = 0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int)\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "                num_iter += 1\n",
        "                # get the inputs\n",
        "                if data == -1: # bracewell does not compile opencv with ffmpeg, strange errors occur resulting in no video loaded\n",
        "                    continue\n",
        "\n",
        "                # inputs, labels, vid, src = data\n",
        "                inputs, labels, vid = data\n",
        "\n",
        "                # wrap them in Variable\n",
        "                inputs = inputs.cuda()\n",
        "                t = inputs.size(2)\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                per_frame_logits = i3d(inputs, pretrained=False)\n",
        "                # upsample to input size\n",
        "                per_frame_logits = F.upsample(per_frame_logits, t, mode='linear')\n",
        "\n",
        "                # compute localization loss\n",
        "                loc_loss = F.binary_cross_entropy_with_logits(per_frame_logits, labels)\n",
        "                tot_loc_loss += loc_loss.data.item()\n",
        "\n",
        "                predictions = torch.max(per_frame_logits, dim=2)[0]\n",
        "                gt = torch.max(labels, dim=2)[0]\n",
        "\n",
        "                # compute classification loss (with max-pooling along time B x C x T)\n",
        "                cls_loss = F.binary_cross_entropy_with_logits(torch.max(per_frame_logits, dim=2)[0],\n",
        "                                                              torch.max(labels, dim=2)[0])\n",
        "                tot_cls_loss += cls_loss.data.item()\n",
        "\n",
        "                for i in range(per_frame_logits.shape[0]):\n",
        "                    confusion_matrix[torch.argmax(gt[i]).item(), torch.argmax(predictions[i]).item()] += 1\n",
        "\n",
        "                loss = (0.5 * loc_loss + 0.5 * cls_loss) / num_steps_per_update\n",
        "                tot_loss += loss.data.item()\n",
        "                if num_iter == num_steps_per_update // 2:\n",
        "                    print(epoch, steps, loss.data.item())\n",
        "                loss.backward()\n",
        "\n",
        "                if num_iter == num_steps_per_update and phase == 'train':\n",
        "                    steps += 1\n",
        "                    num_iter = 0\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    # lr_sched.step()\n",
        "                    if steps % 10 == 0:\n",
        "                        acc = float(np.trace(confusion_matrix)) / np.sum(confusion_matrix)\n",
        "                        print(\n",
        "                            'Epoch {} {} Loc Loss: {:.4f} Cls Loss: {:.4f} Tot Loss: {:.4f} Accu :{:.4f}'.format(epoch,\n",
        "                                                                                                                 phase,\n",
        "                                                                                                                 tot_loc_loss / (10 * num_steps_per_update),\n",
        "                                                                                                                 tot_cls_loss / (10 * num_steps_per_update),\n",
        "                                                                                                                 tot_loss / 10,\n",
        "                                                                                                                 acc))\n",
        "                        tot_loss = tot_loc_loss = tot_cls_loss = 0.\n",
        "            if phase == 'test':\n",
        "                val_score = float(np.trace(confusion_matrix)) / np.sum(confusion_matrix)\n",
        "                if val_score > best_val_score or epoch % 2 == 0:\n",
        "                    best_val_score = val_score\n",
        "                    model_name = save_model + \"nslt_\" + str(num_classes) + \"_\" + str(steps).zfill(\n",
        "                                   6) + '_%3f.pt' % val_score\n",
        "\n",
        "                    torch.save(i3d.module.state_dict(), model_name)\n",
        "                    print(model_name)\n",
        "\n",
        "                print('VALIDATION: {} Loc Loss: {:.4f} Cls Loss: {:.4f} Tot Loss: {:.4f} Accu :{:.4f}'.format(phase,\n",
        "                                                                                                              tot_loc_loss / num_iter,\n",
        "                                                                                                              tot_cls_loss / num_iter,\n",
        "                                                                                                              (tot_loss * num_steps_per_update) / num_iter,\n",
        "                                                                                                              val_score\n",
        "                                                                                                              ))\n",
        "\n",
        "                scheduler.step(tot_loss * num_steps_per_update / num_iter)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # WLASL setting\n",
        "    mode = 'rgb'\n",
        "    root = {'word': '../../data/WLASL2000'}\n",
        "\n",
        "    save_model = 'checkpoints/'\n",
        "    train_split = 'preprocess/nslt_2000.json'\n",
        "\n",
        "    # weights = 'archived/asl2000/FINAL_nslt_2000_iters=5104_top1=32.48_top5=57.31_top10=66.31.pt'\n",
        "    weights = None\n",
        "    config_file = 'configfiles/asl2000.ini'\n",
        "\n",
        "    configs = Config(config_file)\n",
        "    print(root, train_split)\n",
        "    run(configs=configs, mode=mode, root=root, save_model=save_model, train_split=train_split, weights=weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9moLRKt8rfF"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "elIBCmoP6WZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### (1) Check if PyTorch and CUDA are available\n",
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "id": "s5OattKE6W6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### (3) Test PyTorch\n",
        "import torch\n",
        "print(torch.ones(3,2))"
      ],
      "metadata": {
        "id": "xkylbRPs6j2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "WLASL - Kaggle.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}